{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08800c2e",
   "metadata": {},
   "source": [
    "# Use Case\n",
    "This notebook covers the following use case:\n",
    "1. Read NYC Taxi Parquet Data format - List of Parquet files URLs are provided from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "2. For each url (file) perform some transformation and store in Delta format.\n",
    "3. Compute the average distance, average cost per mile and average cost from Delta Table using incremental load\n",
    "4. Store computed value from Step#3 in Delta format into the KPI output folder\n",
    "5. Repeat step 3 to 5 for each month\n",
    "6. Create Delta Table on Delta Format output folder (auto refresh)\n",
    "7. The KPI output folder will have multiple versions of the average distance and the average cost per mile for a trip\n",
    "8. Use Delta Time Travel to present KPI output in a graphical format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcdbc8",
   "metadata": {},
   "source": [
    "## Provide require configurations for the delta lake \n",
    "** If cluster is enabled with delta lake, then we would not require the following configuration cell\n",
    "** Delta Lake Spark Compatibility matrix -  https://docs.delta.io/latest/releases.html, change Delta Lake version based on Spark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\"spark.jars.packages\": \"io.delta:delta-core_2.12:1.0.1,net.andreinc:mockneat:0.4.8\",\n",
    "           \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d2f14",
   "metadata": {},
   "source": [
    "## List of data files \n",
    "** These file URLs are from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b338258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "import java.net.URL\n",
    "import org.apache.commons.io.FileUtils\n",
    "import org.apache.hadoop.fs._\n",
    "\n",
    "// data file object is being used for future reference in order to read parquet files from HDFS\n",
    "case class DataFile(name:String, downloadURL:String, hdfsPath:String)\n",
    "\n",
    "// get Hadoop file system\n",
    "val fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "\n",
    "val fileUrls= List(\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\"/*,,\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet\",\n",
    "\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet\"*/\n",
    ")\n",
    "\n",
    "// Add a file to be downloaded with this Spark job on every node.\n",
    "val listOfDataFile = fileUrls.map(url=>{\n",
    "    val urlPath=url.split(\"/\") \n",
    "    val fileName = urlPath(urlPath.size-1)\n",
    "    val urlSaveFilePath = s\"/tmp/${fileName}\"\n",
    "    val hdfsSaveFilePath = s\"/tmp/${fileName}\"\n",
    "    val file = new File(urlSaveFilePath)\n",
    "    FileUtils.copyURLToFile(new URL(url), file)\n",
    "    // copy local file to HDFS /tmp/${fileName}\n",
    "    // we will use FileSystem.copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst)\n",
    "    fs.copyFromLocalFile(true,true,new org.apache.hadoop.fs.Path(urlSaveFilePath),new org.apache.hadoop.fs.Path(hdfsSaveFilePath))\n",
    "    DataFile(urlPath(urlPath.size-1),url, hdfsSaveFilePath)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36227dd6",
   "metadata": {},
   "source": [
    "##  Create output Directory\n",
    "The location where we would like to create delta format output, change the transformDeltaOutputPath and avgDeltaOutputKPIPath varibale if require\n",
    "- avgDeltaOutputKPIPath - to store average KPI in delta format\n",
    "- transformDeltaOutputPath - store transformed output in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.fs._\n",
    "// create directory/folder if not exist\n",
    "def createDirectory(dataSourcePath: String) = {\n",
    "    val fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "    val path =  new Path(dataSourcePath)\n",
    "    if(!fs.exists(path) && !fs.isDirectory(path)) {\n",
    "        fs.mkdirs(path)\n",
    "    }\n",
    "}\n",
    "val transformDeltaOutputPath = \"/nyctaxideltadata/transform\"\n",
    "val avgDeltaOutputKPIPath = \"/nyctaxideltadata/avgkpi\"\n",
    "createDirectory(transformDeltaOutputPath)\n",
    "createDirectory(avgDeltaOutputKPIPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72bda4",
   "metadata": {},
   "source": [
    "## Create Delta Format Data From Parquet Format\n",
    "\n",
    "- Input data will be from listOfDataFile (data downloaded from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "- To demonstrate the Time travel and version, we will load them individually\n",
    "- Perform transformation and compute following business KPI on incremental load:\n",
    "    - The average distance\n",
    "    - The average cost per mile\n",
    "    - The average cost\n",
    "- Save transformed and KPI data in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8aeb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "// UDF to compute sum of value paid by customer\n",
    "def totalCustPaid = udf((basePassengerFare:Double, tolls:Double,bcf:Double,salesTax:Double,congSurcharge:Double,airportFee:Double, tips:Double) => {\n",
    "    val total = basePassengerFare + tolls + bcf + salesTax + congSurcharge + airportFee + tips\n",
    "    total\n",
    "})\n",
    "\n",
    "// read parquet file from spark conf with given file input\n",
    "// transform data to compute total amount\n",
    "// compute kpi for the given file/batch data\n",
    "def readTransformWriteDelta(fileName:String, oldData:Option[DataFrame], format:String=\"parquet\"):DataFrame = {\n",
    "    val df = spark.read.format(format).load(fileName)\n",
    "    val dfNewLoad= df.withColumn(\"total_amount\",totalCustPaid($\"base_passenger_fare\",$\"tolls\",$\"bcf\",$\"sales_tax\",$\"congestion_surcharge\",$\"airport_fee\",$\"tips\"))\n",
    "    // union with old data to compute KPI\n",
    "    val dfFullLoad= oldData match {\n",
    "        case Some(odf)=>\n",
    "                dfNewLoad.union(odf)\n",
    "        case _ =>\n",
    "                dfNewLoad\n",
    "    }\n",
    "    dfFullLoad.createOrReplaceTempView(\"tempFullLoadCompute\")\n",
    "    val dfKpiCompute = spark.sql(\"SELECT round(avg(trip_miles),2) AS avgDist,round(avg(total_amount/trip_miles),2) AS avgCostPerMile,round(avg(total_amount),2) avgCost FROM tempFullLoadCompute\")\n",
    "    // save only new transformed data\n",
    "    dfNewLoad.write.mode(\"overwrite\").format(\"delta\").save(transformDeltaOutputPath)\n",
    "    //save compute KPI\n",
    "    dfKpiCompute.write.mode(\"overwrite\").format(\"delta\").save(avgDeltaOutputKPIPath)\n",
    "    // return incremental dataframe for next set of load\n",
    "    dfFullLoad\n",
    "}\n",
    "\n",
    "// load data for each data file, use last dataframe for KPI compute with the current load\n",
    "def loadData(dataFile: List[DataFile], oldDF:Option[DataFrame]):Boolean = {\n",
    "    if(dataFile.isEmpty) {\n",
    "        true\n",
    "    } else {\n",
    "        val nextDataFile = dataFile.head\n",
    "        val newFullDF = readTransformWriteDelta(nextDataFile.hdfsPath,oldDF)\n",
    "        loadData(dataFile.tail,Some(newFullDF))\n",
    "    }\n",
    "}\n",
    "loadData(listOfDataFile,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae6226",
   "metadata": {},
   "source": [
    "## Read delta format using Delta Table\n",
    "- read transformed data\n",
    "- read KPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io.delta.tables._\n",
    "val dtTransformed: io.delta.tables.DeltaTable = DeltaTable.forPath(transformDeltaOutputPath)\n",
    "val dtAvgKpi: io.delta.tables.DeltaTable = DeltaTable.forPath(avgDeltaOutputKPIPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b8b2c",
   "metadata": {},
   "source": [
    "## Print Schema\n",
    "Print Delta Table Schema for transformed and average KPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "// tranform data schema\n",
    "dtTransformed.toDF.printSchema\n",
    "// Average KPI Data Schema\n",
    "dtAvgKpi.toDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acddd96",
   "metadata": {},
   "source": [
    "## Display Last Computed KPI from Data Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtAvgKpi.toDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065928b3",
   "metadata": {},
   "source": [
    "## Display Computed KPI History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtAvgKpi.history().show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a560d",
   "metadata": {},
   "source": [
    "## Display KPI data after each data load\n",
    "Using Time travel you can view KPI changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21542c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val kpiAvgLogDF = spark.read.json(s\"${avgDeltaOutputKPIPath}/_delta_log/*.json\")\n",
    "val kpiAvgLogDetailDF = kpiAvgLogDF.select(col(\"add\")(\"path\").alias(\"file_path\")).withColumn(\"version\",substring(input_file_name(),-6,1)).filter(\"file_path is not NULL\")\n",
    "val kpiParquetTableDF = spark.read.parquet(s\"$avgDeltaOutputKPIPath/*.parquet\").withColumn(\"input_file\",substring_index(input_file_name, \"/\", -1))\n",
    "kpiParquetTableDF.join(kpiAvgLogDetailDF,kpiParquetTableDF(\"input_file\") === kpiAvgLogDetailDF(\"file_path\"),\"inner\" ).select(\"avgDist\",\"avgCostPerMile\",\"avgCost\",\"version\").orderBy(\"version\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcf379",
   "metadata": {},
   "source": [
    "## Delta Log for transformed data\n",
    "Query the .json _delta_log transaction files which will tell us which version has added which file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f309bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfTxLog = spark.read.json(s\"${transformDeltaOutputPath}/_delta_log/*.json\")\n",
    "dfTxLog.select(col(\"add\")(\"path\").alias(\"file_path\")).withColumn(\"version\",substring(input_file_name(),-6,1)).filter(\"file_path is not NULL\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

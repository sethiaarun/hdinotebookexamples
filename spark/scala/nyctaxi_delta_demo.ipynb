{"nbformat_minor": 5, "cells": [{"source": "# Use Case\nThis notebook covers the following use case:\n1. Read NYC Taxi Parquet Data format - List of Parquet files URLs are provided from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n2. For each url (file) perform some transformation and store in Delta format.\n3. Compute the average distance, average cost per mile and average cost from Delta Table using incremental load\n4. Store computed value from Step#3 in Delta format into the KPI output folder\n5. Repeat step 3 to 5 for each month\n6. Create Delta Table on Delta Format output folder (auto refresh)\n7. The KPI output folder will have multiple versions of the average distance and the average cost per mile for a trip\n8. Use Delta Time Travel to present KPI output in a graphical format", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## Provide require configurations for the delta lake \n** If cluster is enabled with delta lake, then we would not require the following configuration cell\n** Delta Lake Spark Compatibility matrix -  https://docs.delta.io/latest/releases.html, change Delta Lake version based on Spark Version", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"io.delta:delta-core_2.12:1.0.1,net.andreinc:mockneat:0.4.8\",\n           \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n          }\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 98.110107421875, "end_time": 1680027356965.121}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## List of data files \n** These file URLs are from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import java.io.File\nimport java.net.URL\nimport org.apache.commons.io.FileUtils\nimport org.apache.hadoop.fs._\n\n// data file object is being used for future reference in order to read parquet files from HDFS\ncase class DataFile(name:String, downloadURL:String, hdfsPath:String)\n\n// get Hadoop file system\nval fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n\nval fileUrls= List(\n\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\",\n\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\",\n\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\",\n\"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\"\n)\n\n// Add a file to be downloaded with this Spark job on every node.\nval listOfDataFile = fileUrls.map(url=>{\n    val urlPath=url.split(\"/\") \n    val fileName = urlPath(urlPath.size-1)\n    val urlSaveFilePath = s\"/tmp/${fileName}\"\n    val hdfsSaveFilePath = s\"/tmp/${fileName}\"\n    val file = new File(urlSaveFilePath)\n    FileUtils.copyURLToFile(new URL(url), file)\n    // copy local file to HDFS /tmp/${fileName}\n    // we will use FileSystem.copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst)\n    fs.copyFromLocalFile(true,true,new org.apache.hadoop.fs.Path(urlSaveFilePath),new org.apache.hadoop.fs.Path(hdfsSaveFilePath))\n    DataFile(urlPath(urlPath.size-1),url, hdfsSaveFilePath)\n})", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 35481.43505859375, "end_time": 1680027448694.309}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "##  Create output Directory\nThe location where we would like to create delta format output, change the transformDeltaOutputPath and avgDeltaOutputKPIPath varibale if require\n- avgDeltaOutputKPIPath - to store average KPI in delta format\n- transformDeltaOutputPath - store transformed output in delta format", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.hadoop.fs._\n\n// this is used to store source data being transformed and stored delta format\nval transformDeltaOutputPath = \"/nyctaxideltadata/transform\"\n// this is used to store Average KPI data in delta format\nval avgDeltaOutputKPIPath = \"/nyctaxideltadata/avgkpi\"\n// this is used for POWER BI reporting to show Month on Month change in KPI (not in delta format)\nval avgMoMKPIChangePath = \"/nyctaxideltadata/avgMoMKPIChangePath\"\n\n// create directory/folder if not exist\ndef createDirectory(dataSourcePath: String) = {\n    val fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n    val path =  new Path(dataSourcePath)\n    if(!fs.exists(path) && !fs.isDirectory(path)) {\n        fs.mkdirs(path)\n    }\n}\n\ncreateDirectory(transformDeltaOutputPath)\ncreateDirectory(avgDeltaOutputKPIPath)\ncreateDirectory(avgMoMKPIChangePath)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2285.2841796875, "end_time": 1680027763515.514}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Create Delta Format Data From Parquet Format\n\n- Input data will be from listOfDataFile (data downloaded from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n- To demonstrate the Time travel and version, we will load them individually\n- Perform transformation and compute following business KPI on incremental load:\n    - The average distance\n    - The average cost per mile\n    - The average cost\n- Save transformed and KPI data in delta format", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.DataFrame\n\n// UDF to compute sum of value paid by customer\ndef totalCustPaid = udf((basePassengerFare:Double, tolls:Double,bcf:Double,salesTax:Double,congSurcharge:Double,airportFee:Double, tips:Double) => {\n    val total = basePassengerFare + tolls + bcf + salesTax + congSurcharge + airportFee + tips\n    total\n})\n\n// read parquet file from spark conf with given file input\n// transform data to compute total amount\n// compute kpi for the given file/batch data\ndef readTransformWriteDelta(fileName:String, oldData:Option[DataFrame], format:String=\"parquet\"):DataFrame = {\n    val df = spark.read.format(format).load(fileName)\n    val dfNewLoad= df.withColumn(\"total_amount\",totalCustPaid($\"base_passenger_fare\",$\"tolls\",$\"bcf\",$\"sales_tax\",$\"congestion_surcharge\",$\"airport_fee\",$\"tips\"))\n    // union with old data to compute KPI\n    val dfFullLoad= oldData match {\n        case Some(odf)=>\n                dfNewLoad.union(odf)\n        case _ =>\n                dfNewLoad\n    }\n    dfFullLoad.createOrReplaceTempView(\"tempFullLoadCompute\")\n    val dfKpiCompute = spark.sql(\"SELECT round(avg(trip_miles),2) AS avgDist,round(avg(total_amount/trip_miles),2) AS avgCostPerMile,round(avg(total_amount),2) avgCost FROM tempFullLoadCompute\")\n    // save only new transformed data\n    dfNewLoad.write.mode(\"overwrite\").format(\"delta\").save(transformDeltaOutputPath)\n    //save compute KPI\n    dfKpiCompute.write.mode(\"overwrite\").format(\"delta\").save(avgDeltaOutputKPIPath)\n    // return incremental dataframe for next set of load\n    dfFullLoad\n}\n\n// load data for each data file, use last dataframe for KPI compute with the current load\ndef loadData(dataFile: List[DataFile], oldDF:Option[DataFrame]):Boolean = {\n    if(dataFile.isEmpty) {\n        true\n    } else {\n        val nextDataFile = dataFile.head\n        val newFullDF = readTransformWriteDelta(nextDataFile.hdfsPath,oldDF)\n        loadData(dataFile.tail,Some(newFullDF))\n    }\n}\nloadData(listOfDataFile,None)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 407428.7578125, "end_time": 1680028176641.564}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Read delta format using Delta Table\n- read transformed data\n- read KPI data", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import io.delta.tables._\nval dtTransformed: io.delta.tables.DeltaTable = DeltaTable.forPath(transformDeltaOutputPath)\nval dtAvgKpi: io.delta.tables.DeltaTable = DeltaTable.forPath(avgDeltaOutputKPIPath)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1259.965087890625, "end_time": 1680028177913.095}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Print Schema\nPrint Delta Table Schema for transformed and average KPI data", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// tranform data schema\ndtTransformed.toDF.printSchema\n// Average KPI Data Schema\ndtAvgKpi.toDF.printSchema", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 746.033935546875, "end_time": 1680028968575.149}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Display Last Computed KPI from Data Table ", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "dtAvgKpi.toDF.show(false)", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "## Display Computed KPI History\nThis will display history of KPI transaction table from _delta_log", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "dtAvgKpi.history().show(false)", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "## Display KPI data after each data load\n- Using Time travel you can view KPI changes after each load\n- We are storing all version changes in CSV format at avgMoMKPIChangePath , so that Power BI can read these changes", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.SaveMode._\nval kpiAvgLogDF = spark.read.json(s\"${avgDeltaOutputKPIPath}/_delta_log/*.json\")\nval kpiAvgLogDetailDF = kpiAvgLogDF.select(col(\"add\")(\"path\").alias(\"file_path\")).withColumn(\"version\",substring(input_file_name(),-6,1)).filter(\"file_path is not NULL\")\nval kpiParquetTableDF = spark.read.parquet(s\"$avgDeltaOutputKPIPath/*.parquet\").withColumn(\"input_file\",substring_index(input_file_name, \"/\", -1))\nval avgMoMKPIChangeDF = kpiParquetTableDF.join(kpiAvgLogDetailDF,kpiParquetTableDF(\"input_file\") === kpiAvgLogDetailDF(\"file_path\"),\"inner\" ).select(\"avgDist\",\"avgCostPerMile\",\"avgCost\",\"version\").withColumnRenamed(\"version\",\"month\").orderBy(\"month\")\navgMoMKPIChangeDF.write.mode(Overwrite).format(\"delta\").save(avgMoMKPIChangePath)\navgMoMKPIChangeDF.show(false)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7301.760009765625, "end_time": 1680032189424.66}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Delta Log for transformed data\nQuery the .json _delta_log transaction files which will tell us which version has added which file ", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val dfTxLog = spark.read.json(s\"${transformDeltaOutputPath}/_delta_log/*.json\")\ndfTxLog.select(col(\"add\")(\"path\").alias(\"file_path\")).withColumn(\"version\",substring(input_file_name(),-6,1)).filter(\"file_path is not NULL\").show(false)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1254.27685546875, "end_time": 1680028981695.73}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}
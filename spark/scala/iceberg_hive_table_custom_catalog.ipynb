{"nbformat_minor": 2, "cells": [{"source": "## Iceberg Hive Tables With Custom Catalog", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 778.041015625, "end_time": 1677513423301.862}}, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:1.1.0,io.delta:delta-core_2.12:1.0.1,org.apache.iceberg:iceberg-hive-runtime:1.1.0\",\n           \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\",\n           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n           \"spark.sql.catalog.iceberg\":\"org.apache.iceberg.spark.SparkCatalog\",\n           \"spark.sql.catalog.iceberg.type\":\"hive\",\n           \"spark.sql.catalog.iceberg.warehouse\":\"/iceberg/warehouse\"\n          }\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 769.68994140625, "end_time": 1677513548819.078}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# Spark Catalog Tables\niceberg_table is Iceberg Parquet Table, and spark_table is Spark Parquet Table; both of these will be created in Spark Catalog\n* Assumption: You spark configuration (from Ambari) `metastore.catalog.default` is still using 'spark'\n", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS iceberg_table (id string,\n               creation_date string,\n                 last_update_time string) USING iceberg\"\"\")\nspark.sql(\"\"\"CREATE TABLE IF NOT EXISTS spark_table (id string,\n                creation_date string,\n                last_update_time string)\"\"\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7339.757080078125, "end_time": 1677513929463.694}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nshow tables;", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 11784.282958984375, "end_time": 1677513977228.827}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Iceberg Hive Tables With Custom Catalog\nCreate Customer Iceberg Table in Hive Catalog", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.iceberg.hive.HiveCatalog\nimport org.apache.iceberg.types.Types\nimport org.apache.iceberg.{PartitionSpec, TableProperties, Schema => IcebergSchema}\nimport org.apache.iceberg.CatalogProperties\nimport org.apache.spark.sql.SparkSession\n\nval catalogName = \"iceberg\"\nval nameSpace = \"default\"\nval tableName = \"customer\"\n\ndef createTableByHiveCatalog(spark: SparkSession): Unit = {\n    import scala.collection.JavaConverters._\n    // table specification starts\n    val schema= new IcebergSchema(\n      Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n      Types.NestedField.required(2, \"name\", Types.StringType.get()),\n      Types.NestedField.required(3, \"state\", Types.StringType.get())\n    )\n    val spec = PartitionSpec.builderFor(schema).bucket(\"state\", 128).build()\n    import org.apache.iceberg.catalog.TableIdentifier\n    val tableIdentifier: TableIdentifier = TableIdentifier.of(nameSpace,tableName)\n    val tblProperties = Map(TableProperties.ENGINE_HIVE_ENABLED->\"true\",\"iceberg.catalog\"->\"iceberg\")\n    // table specification ends\n    val catalog = new HiveCatalog()\n    catalog.setConf(spark.sparkContext.hadoopConfiguration)\n    val properties = Map(CatalogProperties.WAREHOUSE_LOCATION->\"/iceberg/warehouse/\")\n    catalog.initialize(catalogName, properties.asJava)\n    catalog.createTable(tableIdentifier, schema, spec,s\"/iceberg/warehouse/${tableName}\",tblProperties.asJava)\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2303.93603515625, "end_time": 1677518071003.611}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//create Hive Catalog Table - External\ncreateTableByHiveCatalog(spark)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1276.579833984375, "end_time": 1677517293400.554}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# Insert Data Into Customer", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nINSERT INTO iceberg.default.customer VALUES (1,\"A\",\"State1\"),(2,\"B\",\"State2\"),(3,\"C\",\"State2\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2287.404052734375, "end_time": 1677517789820.835}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Query Metadata Tables", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nSELECT * FROM iceberg.default.customer.files", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1289.218017578125, "end_time": 1677517797929.656}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Query Using Spark", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val df = spark.table(\"iceberg.default.customer\")\ndf.show()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1285.367919921875, "end_time": 1677517832155.66}}, "editable": true, "collapsed": false, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}
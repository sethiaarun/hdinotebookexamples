{"nbformat_minor": 2, "cells": [{"source": "# Delta Lake Example Code \n\n<span style=\"color:red\">**Only for Spark Workload**</span>\n\nThis notebook is compatible with HDI 5.0 (Spark 3. x and Scala 2.12). In addition, the notebook demonstrates how users can leverage delta lake on the HDI platform. The sample code uses a customer business model, generating random data using [mockneat](https://github.com/nomemory/mockneat).\n\nThe following features are expreienced in this code:\n\n- Configure Delta Lake on HDI\n- Generate Random Data Using MockNeat\n- Write Delta Format\n- Read Delta Format - Spark API and Delta Table\n- Schema evolution\n- Time Travel\n\n\n## Configuration - Spark\n\nWe need to provide the following list of spark configurations for delta lake. \n\n- Add Delta Lake Package and Configure spark.sql.extensions and spark.sql.catalog.spark_catalog", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%configure\n{ \"conf\": {\"spark.jars.packages\": \"io.delta:delta-core_2.12:1.0.1,net.andreinc:mockneat:0.4.8\",\n           \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n          }\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 62.35009765625, "end_time": 1675726151083.828}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Configuration - Data Storage and Generation ", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// define storage path where we would like to save data\nval adsl2Path = \"/tmp/customerdata\"\n// number of data points during iteration1\nval numberOfRecords1=10\n// number of data points during iteration2\nval numberOfRecords2=10", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1300.81103515625, "end_time": 1675726223893.837}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch1 - Generate MockData using MockNeat\n- Use Mockneat for Random Data Generation\n- Generate Customer Data using Mocknet Library\n- Configuration:\n   - numberOfRecords1 - number of records to generate during first cycle", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import net.andreinc.mockneat.MockNeat\nimport net.andreinc.mockneat.abstraction.MockUnit\nimport net.andreinc.mockneat.types.enums.RandomType\nimport java.time.LocalDate\nimport scala.reflect.ClassTag\n\nval mockNeat = MockNeat.threadLocal()\n\n/**\n* Customer Business Model\n**/\ncase class Customer(var customerId: Int, var customerName: String, var firstName: String,\n                    var lastName: String, var userName: String, var registrationDate: String)\n//configure base on your need\n// this program will run on driver side limit by driver memory\nval DateStart = LocalDate.of(2014, 1, 1)\nval DateEnd = LocalDate.of(2016, 1, 1)\n// start and end index\nval startIndex1 = 1\nval endIndex1 = startIndex1 + numberOfRecords1\n\nval customerData = (startIndex1 to endIndex1).map(i=>{\n    Customer(i,\n             mockNeat.names().full().get(),\n             mockNeat.names().first().get(),\n             mockNeat.names().last().get(),\n             mockNeat.users().get(),\n             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get())\n})", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5349.828125, "end_time": 1675726229260.258}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch1 - Save Data Using Delta Lake Format and Print Schema\n- Configuration\n    - **adsl2Path** - path where we would like to save delta lake data, It can be a full path or relative path. [More details](https://learn.microsoft.com/en-us/azure/hdinsight/overview-azure-storage#hdinsight-storage-architecture)", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 247.60693359375, "end_time": 1669679217925.857}}, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.hadoop.fs._\nimport java.util.Date\nimport scala.collection.immutable.{List=>ScalaList}\n\n/**\n* Object to capture Delta File Detail\n* @param filePath: File Path\n* @param modifiedTime: Modified Time\n*/\ncase class DeltaFileDetail(filePath: Path, modifiedTime: Date) {\n    override def toString(): String = {\n        s\"File : ${filePath.toString()} , Modified Time: ${modifiedTime.toString()}\"\n    }\n}\n\n/**\n* get list of files from Hadoop System\n*/\ndef getListOfFile(path: String):ScalaList[DeltaFileDetail] = {\n  val fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n  fs.listStatus(new Path(s\"${path}\")).filter(!_.isDir).map(fileStatus=> DeltaFileDetail(fileStatus.getPath, new Date(fileStatus.getModificationTime()))).toList\n}\n\n\n//create data frame\nval df = sc.parallelize(customerData).toDF\ndf.write.mode(\"append\").format(\"delta\").save(adsl2Path)\n// print schema of the dataframe\ndf.printSchema", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 38047.408203125, "end_time": 1675726270586.085}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch1 - List Storage Directory (Parquet files and Delta Logs)", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val listOfFiles1 = getListOfFile(adsl2Path)\nprintln(\"------------------------------ Parquet Files---------------------------\")\nlistOfFiles1.foreach(println)\nval listOfLogs1= getListOfFile(adsl2Path + \"/_delta_log\")\nprintln(\"****************************** Delta Logs ******************************\")\nlistOfLogs1.foreach(println)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1289.556884765625, "end_time": 1675726450975.04}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch1- Read Delta Format data from storage\n- We can read using Spark Read API\n- or using [Delta Table API](https://docs.delta.io/latest/api/scala/io/delta/tables/DeltaTable.html)", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// we can use Spark read or delta table\nval df = spark.read.format(\"delta\").load(adsl2Path)\nprintln(s\"***************** number of records : ${df.count()}\")\n// you can use delta table to read (auto refresh) data\nimport io.delta.tables._\nval dt: io.delta.tables.DeltaTable = DeltaTable.forPath(adsl2Path)\n// convert Table to DataFrame\ndt.toDF.show(20)\nprintln(s\"***************** number of records from delta table : ${dt.toDF.count()}\")\n//Delta Table Version History \ndt.history().show(false)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5377.85498046875, "end_time": 1675726498416.215}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch2- Schema Evolution and Data Generation\n\nRemoved UserName from the existing model and added a new column age.\n\nYou can configure how many records will be generated during the second cycle with numberOfRecords2.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/**\n* Customer new Business Model\n* removed user name and added age column\n**/\ncase class CustomerNew(var customerId: Int, var customerName: String, var firstName: String,\n                    var lastName: String, var registrationDate: String, var age:Int)\n//configure base on your need\n// this program will run on driver side limit by driver memory\nval DateStart = LocalDate.of(2014, 1, 1)\nval DateEnd = LocalDate.of(2016, 1, 1)\n// don't change these variables\nval newStartIndex2 = endIndex1+1\nval newendIndex2 = newStartIndex2 + numberOfRecords2\n\n\nval customerNewData = (newStartIndex2 to newendIndex2).map(i=>{\n    CustomerNew(i,\n             mockNeat.names().full().get(),\n             mockNeat.names().first().get(),\n             mockNeat.names().last().get(),\n             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get(),\n             mockNeat.ints().range(10, 100).get())\n})\n\n// create datafarme from mock data\nval df = sc.parallelize(customerNewData).toDF\n//save it in delta format\ndf.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(adsl2Path)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7374.6298828125, "end_time": 1675726521542.966}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch2 - New Files - Parquet and Delta Logs", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "println(\"-------------------------------- New Parquet Files -------------------------------\")\nval listOfFiles2 = getListOfFile(adsl2Path)\nlistOfFiles1.filterNot(listOfFiles2.toSet).foreach(println)\nprintln(\"************************ New Delta Logs ************************ \")\nval listOfLogs2= getListOfFile(adsl2Path + \"/_delta_log\")\nlistOfLogs1.filterNot(listOfLogs2.toSet).foreach(println)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1304.31201171875, "end_time": 1675726675181.196}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Batch2 - Delta Log - Transaction log History", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//the delta table auto refresh capability will get newly written data unlike Spark read where you have to read data again\ndt.toDF.show(30)\n// number of records should increase\nprintln(s\"***************** number of records from delta table : ${dt.toDF.count()}\")\n//Delta Table Version History - new version is added\nprintln(\"************************  delta log history ************************\")\ndt.history().show(false)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5314.427001953125, "end_time": 1675726727356.824}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Time Travel\n Read Version Zero (Initial)", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// load version zero (initial)\nval dfVersion0 = spark.read.format(\"delta\").option(\"versionAsOf\",0).load(adsl2Path)\ndfVersion0.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3360.510986328125, "end_time": 1675726735196.128}}, "editable": true, "collapsed": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}, "celltoolbar": "Raw Cell Format"}}
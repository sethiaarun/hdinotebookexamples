{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Example Code \n",
    "\n",
    "This notebook is compatible with HDI 5.0 (Spark 3. x and Scala 2.12). In addition, the notebook demonstrates how users can leverage delta lake on the HDI platform. The sample code uses a customer business model, generating random data using [mockneat](https://github.com/nomemory/mockneat).\n",
    "\n",
    "The following features are expreienced in this code:\n",
    "\n",
    "- Configure Delta Lake on HDI\n",
    "- Generate Random Data Using MockNeat\n",
    "- Write Delta Format\n",
    "- Read Delta Format\n",
    "- Merge Schema\n",
    "- Time Travel\n",
    "\n",
    "## Configuration\n",
    "\n",
    "We need to provide the following list of spark configurations for delta lake. \n",
    "\n",
    "- Add Delta Lake Package and Configure spark.sql.extensions and spark.sql.catalog.spark_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 18.546875,
      "end_time": 1669694274683.965
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\"spark.jars.packages\": \"io.delta:delta-core_2.12:1.0.1,net.andreinc:mockneat:0.4.8\",\n",
    "           \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate MockData using MockNeat\n",
    "- Use Mockneat for Random Data Generation\n",
    "- Generate Customer Data using Mocknet Library\n",
    "- Configuration:\n",
    "   - numberOfRecords1 - number of records to generate during first cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2282.529052734375,
      "end_time": 1669695646413.114
     }
    }
   },
   "outputs": [],
   "source": [
    "import net.andreinc.mockneat.MockNeat\n",
    "import net.andreinc.mockneat.abstraction.MockUnit\n",
    "import net.andreinc.mockneat.types.enums.RandomType\n",
    "import java.time.LocalDate\n",
    "import scala.reflect.ClassTag\n",
    "\n",
    "val mockNeat = MockNeat.threadLocal()\n",
    "\n",
    "/**\n",
    "* Customer Business Model\n",
    "**/\n",
    "case class Customer(var customerId: Int, var customerName: String, var firstName: String,\n",
    "                    var lastName: String, var userName: String, var registrationDate: String)\n",
    "//configure base on your need\n",
    "// this program will run on driver side limit by driver memory\n",
    "val DateStart = LocalDate.of(2014, 1, 1)\n",
    "val DateEnd = LocalDate.of(2016, 1, 1)\n",
    "// number of mock data to be generated\n",
    "val numberOfRecords1 = 10\n",
    "val startIndex1 = 1\n",
    "val endIndex1 = startIndex1 + numberOfRecords1\n",
    "\n",
    "val customerData = (startIndex1 to endIndex1).map(i=>{\n",
    "    Customer(i,\n",
    "             mockNeat.names().full().get(),\n",
    "             mockNeat.names().first().get(),\n",
    "             mockNeat.names().last().get(),\n",
    "             mockNeat.users().get(),\n",
    "             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.60693359375,
      "end_time": 1669679217925.857
     }
    }
   },
   "source": [
    "## Save Data Using Delta Lake Format and Print Schema\n",
    "- Configuration\n",
    "    - **adsl2Path** - path where we would like to save delta lake data, It can be a full path or relative path. [More details](https://learn.microsoft.com/en-us/azure/hdinsight/overview-azure-storage#hdinsight-storage-architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23455.97900390625,
      "end_time": 1669695291570.074
     }
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.hadoop.fs._\n",
    "import java.util.Date\n",
    "import scala.collection.immutable.{List=>ScalaList}\n",
    "\n",
    "// define Delta Lake Path\n",
    "val adsl2Path = \"/tmp/customerdata5\"\n",
    "\n",
    "/**\n",
    "* Object to capture Delta File Detail\n",
    "* @param filePath: File Path\n",
    "* @param modifiedTime: Modified Time\n",
    "*/\n",
    "case class DeltaFileDetail(filePath: Path, modifiedTime: Date) {\n",
    "    override def toString(): String = {\n",
    "        s\"File : ${filePath.toString()} , Modified Time: ${modifiedTime.toString()}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    "* get list of files from Hadoop System\n",
    "*/\n",
    "def getListOfFile(path: String):ScalaList[DeltaFileDetail] = {\n",
    "  val fs:FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  fs.listStatus(new Path(s\"${path}\")).filter(!_.isDir).map(fileStatus=> DeltaFileDetail(fileStatus.getPath, new Date(fileStatus.getModificationTime()))).toList\n",
    "}\n",
    "\n",
    "\n",
    "//create data frame\n",
    "val df = sc.parallelize(customerData).toDF\n",
    "df.write.mode(\"append\").format(\"delta\").save(adsl2Path)\n",
    "// print schema of the dataframe\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIst Storage Directory (Parquet files and Delta Logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"----------------------------------------------------------------- Parquet Files-----------------------------------------------------------------\")\n",
    "val listOfFiles1 = getListOfFile(adsl2Path)\n",
    "listOfFiles1.foreach(println)\n",
    "println(\"----------------------------------------------------------------- Delta Logs-----------------------------------------------------------------\")\n",
    "val listOfLogs1= getListOfFile(adsl2Path + \"/_delta_log\")\n",
    "listOfLogs1.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Delta Format data from storage\n",
    "- We can read using Spark Read API\n",
    "- or using [Delta Table API](https://docs.delta.io/latest/api/scala/io/delta/tables/DeltaTable.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// we can use Spark read or delta table\n",
    "val df = spark.read.format(\"parquet\").load(adsl2Path)\n",
    "println(s\"***************** number of records : ${df.count()}\")\n",
    "// you can use delta table to read (auto refresh) data\n",
    "import io.delta.tables._\n",
    "val dt: io.delta.tables.DeltaTable = DeltaTable.forPath(adsl2Path)\n",
    "// convert Table to DataFrame\n",
    "dt.toDF.show(20)\n",
    "println(s\"***************** number of records from delta table : ${dt.toDF.count()}\")\n",
    "//Delta Table Version History \n",
    "dt.history().show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Schema Enforcement\n",
    "\n",
    "Removed UserName from the existing model and added a new column age.\n",
    "\n",
    "You can configure how many records will be generated during the second cycle with numberOfRecords2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numberOfRecords2 = 10\n",
    "\n",
    "/**\n",
    "* Customer new Business Model\n",
    "* removed user name and added age column\n",
    "**/\n",
    "case class CustomerNew(var customerId: Int, var customerName: String, var firstName: String,\n",
    "                    var lastName: String, var registrationDate: String, var age:Int)\n",
    "//configure base on your need\n",
    "// this program will run on driver side limit by driver memory\n",
    "val DateStart = LocalDate.of(2014, 1, 1)\n",
    "val DateEnd = LocalDate.of(2016, 1, 1)\n",
    "// don't change these variables\n",
    "val newStartIndex2 = endIndex1+1\n",
    "val newendIndex2 = newStartIndex2 + numberOfRecords2\n",
    "\n",
    "\n",
    "val customerNewData = (newStartIndex2 to newendIndex2).map(i=>{\n",
    "    CustomerNew(i,\n",
    "             mockNeat.names().full().get(),\n",
    "             mockNeat.names().first().get(),\n",
    "             mockNeat.names().last().get(),\n",
    "             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get(),\n",
    "             mockNeat.ints().range(10, 100).get())\n",
    "})\n",
    "\n",
    "// create datafarme from mock data\n",
    "val df = sc.parallelize(customerNewData).toDF\n",
    "//save it in delta format\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(adsl2Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Files - Parquet and Delta Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"----------------------------------------------------------------- Parquet Files-----------------------------------------------------------------\")\n",
    "val listOfFiles2 = getListOfFile(adsl2Path)\n",
    "listOfFiles1.filterNot(listOfFiles2.toSet).foreach(println)\n",
    "println(\"----------------------------------------------------------------- Delta Logs-----------------------------------------------------------------\")\n",
    "val listOfLogs2= getListOfFile(adsl2Path + \"/_delta_log\")\n",
    "listOfLogs1.filterNot(listOfLogs2.toSet).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Log - Transaction log History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//the delta table auto refresh capability will get newly written data unlike Spark read where you have to read data again\n",
    "dt.toDF.show(30)\n",
    "// number of records should increase\n",
    "println(s\"***************** number of records from delta table : ${dt.toDF.count()}\")\n",
    "//Delta Table Version History - new version is added\n",
    "println(\"------------------------  delta log history ------------------------------------\")\n",
    "dt.history().show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    " Read Version Zero (Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load version zero (initial)\n",
    "val dfVersion0 = spark.read.format(\"delta\").option(\"versionAsOf\",0).load(adsl2Path)\n",
    "dfVersion0.count()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

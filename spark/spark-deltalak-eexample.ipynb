{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Add Delta Lake Package and Configure spark.sql.extensions and spark.sql.catalog.spark_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 18.546875,
      "end_time": 1669694274683.965
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\"spark.jars.packages\": \"io.delta:delta-core_2.12:1.0.1,net.andreinc:mockneat:0.4.8\",\n",
    "           \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "           \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate MockData using MockNeat\n",
    "- Use Mockneat for Random Data Generation\n",
    "- Generate Customer Data using Mocknet Library\n",
    "- Configuration:\n",
    "   - numberOfRecords - number of records to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2282.529052734375,
      "end_time": 1669695646413.114
     }
    }
   },
   "outputs": [],
   "source": [
    "import net.andreinc.mockneat.MockNeat\n",
    "import net.andreinc.mockneat.abstraction.MockUnit\n",
    "import net.andreinc.mockneat.types.enums.RandomType\n",
    "import java.time.LocalDate\n",
    "import scala.reflect.ClassTag\n",
    "\n",
    "val mockNeat = MockNeat.threadLocal()\n",
    "\n",
    "/**\n",
    "* Customer Business Model\n",
    "**/\n",
    "case class Customer(var customerId: Int, var customerName: String, var firstName: String,\n",
    "                    var lastName: String, var userName: String, var registrationDate: String)\n",
    "//configure base on your need\n",
    "// this program will run on driver side limit by driver memory\n",
    "val DateStart = LocalDate.of(2014, 1, 1)\n",
    "val DateEnd = LocalDate.of(2016, 1, 1)\n",
    "// number of mock data to be generated\n",
    "val numberOfRecords1 = 10\n",
    "val startIndex1 = 1\n",
    "val endIndex1 = startIndex1 + numberOfRecords1\n",
    "\n",
    "val customerData = (startIndex1 to endIndex1).map(i=>{\n",
    "    Customer(i,\n",
    "             mockNeat.names().full().get(),\n",
    "             mockNeat.names().first().get(),\n",
    "             mockNeat.names().last().get(),\n",
    "             mockNeat.users().get(),\n",
    "             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.60693359375,
      "end_time": 1669679217925.857
     }
    }
   },
   "source": [
    "## Delta Lake Save Data and Print Schema\n",
    "- Configuration\n",
    "    - adsl2Path - path where we would like to save delta lake data, It can be a full path or relative path. [More details](https://learn.microsoft.com/en-us/azure/hdinsight/overview-azure-storage#hdinsight-storage-architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23455.97900390625,
      "end_time": 1669695291570.074
     }
    }
   },
   "outputs": [],
   "source": [
    "// define Delta Lake Path\n",
    "val adsl2Path = \"/tmp/customerdata3\"\n",
    "\n",
    "//create data frame\n",
    "val df = sc.parallelize(customerData).toDF\n",
    "df.write.mode(\"append\").format(\"delta\").save(adsl2Path)\n",
    "// print schema of the dataframe\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Schema Enforcement\n",
    "Removed UserName from existing model and added a new column age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "* Customer new Business Model\n",
    "* removed user name and added age column\n",
    "**/\n",
    "case class CustomerNew(var customerId: Int, var customerName: String, var firstName: String,\n",
    "                    var lastName: String, var registrationDate: String, var age:Int)\n",
    "//configure base on your need\n",
    "// this program will run on driver side limit by driver memory\n",
    "val DateStart = LocalDate.of(2014, 1, 1)\n",
    "val DateEnd = LocalDate.of(2016, 1, 1)\n",
    "// number of mock data to be generated\n",
    "val newStartIndex2 = endIndex1+1\n",
    "val numberOfRecords2 = 10\n",
    "val newendIndex2 = newStartIndex2 + numberOfRecords2\n",
    "\n",
    "\n",
    "val customerNewData = (newStartIndex2 to newendIndex2).map(i=>{\n",
    "    CustomerNew(i,\n",
    "             mockNeat.names().full().get(),\n",
    "             mockNeat.names().first().get(),\n",
    "             mockNeat.names().last().get(),\n",
    "             mockNeat.localDates.between(DateStart, DateEnd).mapToString().get(),\n",
    "             mockNeat.ints().range(10, 100).get())\n",
    "})\n",
    "\n",
    "// create datafarme from mock data\n",
    "val df = sc.parallelize(customerNewData).toDF\n",
    "//save it in delta format\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(adsl2Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Delta Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// we can use Spark read or delta table\n",
    "val df = spark.read.format(\"parquet\").load(adsl2Path)\n",
    "df.show(20)\n",
    "\n",
    "// you can use delta table to read (auto refresh) data\n",
    "import io.delta.tables._\n",
    "val dt: io.delta.tables.DeltaTable = DeltaTable.forPath(adsl2Path)\n",
    "dt.toDF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
